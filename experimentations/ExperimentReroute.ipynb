{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ade79d8-9268-417a-a310-be383a199414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "P4 compilation failed:\n",
      "rerouteDeep.p4(81): [--Werror=type-error] error: drop_counter.read: extern register does not have method matching this call\n",
      "...ter.write(standard_metadata.ingress_port, drop_counter.read(standard_metadata.ingress_port) + 1);\n",
      "                                             ^^^^^^^^^^^^^^^^^\n",
      "/usr/local/share/p4c/p4include/v1model.p4(292)\n",
      "extern register<T>\n",
      "       ^^^^^^^^\n",
      "rerouteDeep.p4(89): [--Werror=type-error] error: 'active_port.write(standard_metadata.ingress_port, port)'\n",
      "        active_port.write(standard_metadata.ingress_port, port);\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  ---- Actual error:\n",
      "  Cannot cast implicitly type 'bit<9>' to type 'bit<32>'\n",
      "  ---- Originating from:\n",
      "  rerouteDeep.p4(89): Type of argument 'standard_metadata.ingress_port' (bit<9>) does not match type of parameter 'index' (bit<32>)\n",
      "          active_port.write(standard_metadata.ingress_port, port);\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  /usr/local/share/p4c/p4include/v1model.p4(351)\n",
      "      void write(in bit<32> index, in T value);\n",
      "                            ^^^^^\n",
      "  ---- Originating from:\n",
      "  /usr/local/share/p4c/p4include/v1model.p4(351): Function type 'write' does not match invocation type '<Method call>'\n",
      "      void write(in bit<32> index, in T value);\n",
      "           ^^^^^\n",
      "  rerouteDeep.p4(89)\n",
      "          active_port.write(standard_metadata.ingress_port, port);\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "rerouteDeep.p4(90): [--Werror=type-error] error: 'h1_to_h2_active_port.write(standard_metadata.ingress_port, port)'\n",
      "        h1_to_h2_active_port.write(standard_metadata.ingress_port, port);\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  ---- Actual error:\n",
      "  Cannot cast implicitly type 'bit<9>' to type 'bit<32>'\n",
      "  ---- Originating from:\n",
      "  rerouteDeep.p4(90): Type of argument 'standard_metadata.ingress_port' (bit<9>) does not match type of parameter 'index' (bit<32>)\n",
      "          h1_to_h2_active_port.write(standard_metadata.ingress_port, port);\n",
      "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  /usr/local/share/p4c/p4include/v1model.p4(351)\n",
      "      void write(in bit<32> index, in T value);\n",
      "                            ^^^^^\n",
      "  ---- Originating from:\n",
      "  /usr/local/share/p4c/p4include/v1model.p4(351): Function type 'write' does not match invocation type '<Method call>'\n",
      "      void write(in bit<32> index, in T value);\n",
      "           ^^^^^\n",
      "  rerouteDeep.p4(90)\n",
      "          h1_to_h2_active_port.write(standard_metadata.ingress_port, port);\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "Failed to compile P4 program\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from mininet.net import Mininet\n",
    "from mininet.topo import Topo\n",
    "from mininet.node import Switch\n",
    "from mininet.link import TCLink\n",
    "from mininet.cli import CLI\n",
    "from mininet.log import setLogLevel, info, error\n",
    "\n",
    "class P4Switch(Switch):\n",
    "    def __init__(self, name, json_file, thrift_port, device_id, **params):\n",
    "        Switch.__init__(self, name, **params)\n",
    "        self.json_file = json_file\n",
    "        self.thrift_port = thrift_port\n",
    "        self.device_id = device_id\n",
    "        self.proc = None\n",
    "\n",
    "    def start(self, controllers):\n",
    "        cmd = [\n",
    "            'simple_switch',\n",
    "            '--thrift-port', str(self.thrift_port),\n",
    "            '--device-id', str(self.device_id),\n",
    "            '--nanolog', f'ipc:///tmp/bm-{self.device_id}-log.ipc',\n",
    "            '--log-console',\n",
    "            self.json_file\n",
    "        ]\n",
    "        for idx, intf in enumerate(self.intfList()):\n",
    "            if intf.name != \"lo\":\n",
    "                cmd.extend(['-i', f\"{idx+1}@{intf.name}\"])\n",
    "        info(f\"Starting P4 switch {self.name}: {' '.join(cmd)}\\n\")\n",
    "        self.proc = subprocess.Popen(\n",
    "            cmd,\n",
    "            stdout=open(f'{self.name}.log', 'w'),\n",
    "            stderr=subprocess.STDOUT\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "    def stop(self):\n",
    "        if self.proc:\n",
    "            self.proc.terminate()\n",
    "            self.proc.wait()\n",
    "\n",
    "class FastRerouteTopo(Topo):\n",
    "    def __init__(self, json_file):\n",
    "        Topo.__init__(self)\n",
    "        switches = []\n",
    "        for i in range(1, 4):\n",
    "            sw = self.addSwitch(\n",
    "                f's{i}',\n",
    "                cls=P4Switch,\n",
    "                json_file=json_file,\n",
    "                thrift_port=9090 + (i-1),\n",
    "                device_id=i\n",
    "            )\n",
    "            switches.append(sw)\n",
    "\n",
    "        hosts = []\n",
    "        for i in range(1, 4):\n",
    "            host = self.addHost(\n",
    "                f'h{i}',\n",
    "                ip=f'10.0.{i}.1/24',\n",
    "                mac=f'00:00:00:00:0{i}:01',\n",
    "                defaultRoute=f'via 10.0.{i}.254'\n",
    "            )\n",
    "            hosts.append(host)\n",
    "            self.addLink(host, switches[i-1], port1=0, port2=2, cls=TCLink, bw=100, delay='1ms')\n",
    "\n",
    "        # Primary links\n",
    "        self.addLink(switches[0], switches[1], port1=3, port2=3, cls=TCLink, bw=50, delay='1ms')  # s1-s2\n",
    "        self.addLink(switches[1], switches[2], port1=4, port2=3, cls=TCLink, bw=50, delay='1ms')  # s2-s3\n",
    "        self.addLink(switches[2], switches[0], port1=4, port2=4, cls=TCLink, bw=50, delay='1ms')  # s3-s1\n",
    "\n",
    "        # Backup links\n",
    "        self.addLink(switches[0], switches[1], port1=5, port2=5, cls=TCLink, bw=30, delay='10ms')  # s1-s2\n",
    "        self.addLink(switches[1], switches[2], port1=6, port2=5, cls=TCLink, bw=30, delay='10ms')  # s2-s3\n",
    "        self.addLink(switches[2], switches[0], port1=6, port2=6, cls=TCLink, bw=30, delay='10ms')  # s3-s1\n",
    "\n",
    "def configure_switch(sw):\n",
    "    thrift_port = sw.thrift_port\n",
    "    config = [\n",
    "        'register_write link_status 2 1',  # Port to host\n",
    "        'register_write link_status 3 1',  # Primary port\n",
    "        'register_write link_status 4 1',  # Primary port\n",
    "        'register_write link_status 5 1',  # Backup port\n",
    "        'register_write link_status 6 1',  # Backup port\n",
    "    ]\n",
    "\n",
    "    if sw.name == 's1':\n",
    "        config += [\n",
    "            'table_add arp_table generate_arp_reply 10.0.1.254 => 00:00:00:00:01:01 10.0.1.254',\n",
    "            'table_add ipv4_lpm set_routes 10.0.1.1/32 => 2 00:00:00:00:01:01 0 00:00:00:00:00:00 00:00:00:00:01:01',\n",
    "            'table_add ipv4_lpm set_routes 10.0.2.1/32 => 3 00:00:00:00:02:02 5 00:00:00:00:02:04 00:00:00:00:01:02',\n",
    "            'table_add ipv4_lpm set_routes 10.0.3.1/32 => 4 00:00:00:00:03:03 6 00:00:00:00:03:06 00:00:00:00:01:03',\n",
    "            'table_add backup_routes set_backup_nhop 10.0.2.1/32 => 00:00:00:00:02:04 5 00:00:00:00:01:04',\n",
    "            'table_add backup_routes set_backup_nhop 10.0.3.1/32 => 00:00:00:00:03:06 6 00:00:00:00:01:06',\n",
    "        ]\n",
    "    elif sw.name == 's2':\n",
    "        config += [\n",
    "            'table_add arp_table generate_arp_reply 10.0.2.254 => 00:00:00:00:02:01 10.0.2.254',\n",
    "            'table_add ipv4_lpm set_routes 10.0.2.1/32 => 2 00:00:00:00:02:01 0 00:00:00:00:00:00 00:00:00:00:02:01',\n",
    "            'table_add ipv4_lpm set_routes 10.0.3.1/32 => 4 00:00:00:00:03:03 6 00:00:00:00:03:05 00:00:00:00:02:03',\n",
    "            'table_add ipv4_lpm set_routes 10.0.1.1/32 => 3 00:00:00:00:01:02 5 00:00:00:00:01:04 00:00:00:00:02:02',\n",
    "            'table_add backup_routes set_backup_nhop 10.0.3.1/32 => 00:00:00:00:03:05 6 00:00:00:00:02:05',\n",
    "            'table_add backup_routes set_backup_nhop 10.0.1.1/32 => 00:00:00:00:01:04 5 00:00:00:00:02:04',\n",
    "        ]\n",
    "    elif sw.name == 's3':\n",
    "        config += [\n",
    "            'table_add arp_table generate_arp_reply 10.0.3.254 => 00:00:00:00:03:01 10.0.3.254',\n",
    "            'table_add ipv4_lpm set_routes 10.0.3.1/32 => 2 00:00:00:00:03:01 0 00:00:00:00:00:00 00:00:00:00:03:01',\n",
    "            'table_add ipv4_lpm set_routes 10.0.1.1/32 => 4 00:00:00:00:01:03 6 00:00:00:00:01:06 00:00:00:00:03:02',\n",
    "            'table_add ipv4_lpm set_routes 10.0.2.1/32 => 3 00:00:00:00:02:03 5 00:00:00:00:02:05 00:00:00:00:03:03',\n",
    "            'table_add backup_routes set_backup_nhop 10.0.1.1/32 => 00:00:00:00:01:06 6 00:00:00:00:03:06',\n",
    "            'table_add backup_routes set_backup_nhop 10.0.2.1/32 => 00:00:00:00:02:05 5 00:00:00:00:03:05',\n",
    "        ]\n",
    "\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            ['simple_switch_CLI', '--thrift-port', str(thrift_port)],\n",
    "            input='\\n'.join(config),\n",
    "            text=True,\n",
    "            check=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        info(f\"Configured switch {sw.name} successfully\\n\")\n",
    "    except subprocess.SubprocessError as e:\n",
    "        error(f\"Failed to configure {sw.name}: {str(e)}\\n\")\n",
    "\n",
    "def read_registers(net, switch_name, thrift_port, output_file):\n",
    "    info(f\"\\n*** {switch_name} Registers:\\n\")\n",
    "    with open(output_file, 'w') as f:\n",
    "        subprocess.run(f\"echo 'register_read link_status' | simple_switch_CLI --thrift-port {thrift_port}\", shell=True, stdout=f)\n",
    "        subprocess.run(f\"echo 'register_read backup_counter' | simple_switch_CLI --thrift-port {thrift_port}\", shell=True, stdout=f)\n",
    "        subprocess.run(f\"echo 'register_read drop_counter' | simple_switch_CLI --thrift-port {thrift_port}\", shell=True, stdout=f)\n",
    "        subprocess.run(f\"echo 'register_read active_port' | simple_switch_CLI --thrift-port {thrift_port}\", shell=True, stdout=f)\n",
    "        subprocess.run(f\"echo 'register_read h1_to_h2_active_port' | simple_switch_CLI --thrift-port {thrift_port}\", shell=True, stdout=f)\n",
    "    with open(output_file, 'r') as f:\n",
    "        info(f.read())\n",
    "\n",
    "def log_registers_periodically(net, switch_name, thrift_port, base_filename):\n",
    "    for i in range(20):  # Log every 2 seconds for 40 seconds\n",
    "        time.sleep(2)\n",
    "        timestamp = i * 2\n",
    "        output_file = f\"{base_filename}_t{timestamp}.txt\"\n",
    "        info(f\"*** Logging {switch_name} Registers at {timestamp} seconds\\n\")\n",
    "        with open(output_file, 'w') as f:\n",
    "            subprocess.run(f\"echo 'register_read h1_to_h2_active_port' | simple_switch_CLI --thrift-port {thrift_port}\", shell=True, stdout=f)\n",
    "        with open(output_file, 'r') as f:\n",
    "            info(f.read())\n",
    "\n",
    "def compile_p4_program(p4_file):\n",
    "    json_file = p4_file.replace('.p4', '.json')\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['p4c-bm2-ss', '--std', 'p4-16', '-o', json_file, p4_file],\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        info(\"P4 compilation succeeded:\\n\" + result.stderr + \"\\n\")\n",
    "        return json_file\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        error(\"P4 compilation failed:\\n\" + e.stderr + \"\\n\")\n",
    "        return None\n",
    "\n",
    "def execute_switch_cmd(net, switch_name, thrift_port, cmd):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['simple_switch_CLI', '--thrift-port', str(thrift_port)],\n",
    "            input=cmd,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "            check=True,\n",
    "            timeout=5\n",
    "        )\n",
    "        info(f\"Executed '{cmd}' on {switch_name}: {result.stdout}\\n\")\n",
    "        return True\n",
    "    except subprocess.SubprocessError as e:\n",
    "        error(f\"Failed to execute '{cmd}' on {switch_name}: {e.stderr}\\n\")\n",
    "        return False\n",
    "\n",
    "def verify_link_delays(net):\n",
    "    info(\"*** Verifying link delays\\n\")\n",
    "    s1 = net.get('s1')\n",
    "    s2 = net.get('s2')\n",
    "    # Verify primary link (should be 1ms)\n",
    "    info(\"Primary link s1-eth3 delay:\\n\")\n",
    "    s1.cmdPrint('tc qdisc show dev s1-eth3')\n",
    "    info(\"Primary link s2-eth3 delay:\\n\")\n",
    "    s2.cmdPrint('tc qdisc show dev s2-eth3')\n",
    "    # Verify backup link (should be 10ms)\n",
    "    info(\"Backup link s1-eth5 delay:\\n\")\n",
    "    s1.cmdPrint('tc qdisc show dev s1-eth5')\n",
    "    info(\"Backup link s2-eth5 delay:\\n\")\n",
    "    s2.cmdPrint('tc qdisc show dev s2-eth5')\n",
    "    # Ensure backup link delay is set correctly\n",
    "    s1.cmd('tc qdisc replace dev s1-eth5 root netem delay 10ms')\n",
    "    s2.cmd('tc qdisc replace dev s2-eth5 root netem delay 10ms')\n",
    "    info(\"Backup link delays set to 10ms\\n\")\n",
    "\n",
    "def main():\n",
    "    setLogLevel('info')\n",
    "    os.system('pkill -f \"simple_switch|bmv2\"')\n",
    "    \n",
    "    json_file = compile_p4_program(\"rerouteDeep.p4\")\n",
    "    if not json_file:\n",
    "        error(\"Failed to compile P4 program\\n\")\n",
    "        return\n",
    "\n",
    "    net = Mininet(\n",
    "        topo=FastRerouteTopo(json_file),\n",
    "        controller=None,\n",
    "        link=TCLink,\n",
    "        autoSetMacs=True\n",
    "    )\n",
    "    net.start()\n",
    "\n",
    "    try:\n",
    "        # Configure switches\n",
    "        for sw in net.switches:\n",
    "            configure_switch(sw)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        # Ensure hosts respond to pings\n",
    "        for host in net.hosts:\n",
    "            host.cmd('sysctl -w net.ipv4.icmp_echo_ignore_all=0 >/dev/null')\n",
    "\n",
    "        # Preconfigure ARP to avoid delays\n",
    "        h1 = net.get('h1')\n",
    "        h2 = net.get('h2')\n",
    "        h3 = net.get('h3')\n",
    "        h1.cmd('arp -s 10.0.2.254 00:00:00:00:02:01')\n",
    "        h2.cmd('arp -s 10.0.1.254 00:00:00:00:01:01')\n",
    "        h3.cmd('arp -s 10.0.1.254 00:00:00:00:01:01')\n",
    "        h3.cmd('arp -s 10.0.2.254 00:00:00:00:02:01')\n",
    "        info(\"*** Preconfigured ARP entries on hosts\\n\")\n",
    "\n",
    "        # Verify link delays\n",
    "        verify_link_delays(net)\n",
    "\n",
    "        # Initial state\n",
    "        info(\"*** Initial Register State\\n\")\n",
    "        read_registers(net, 's1', 9090, 's1_initial_regs.txt')\n",
    "        read_registers(net, 's2', 9091, 's2_initial_regs.txt')\n",
    "        read_registers(net, 's3', 9092, 's3_initial_regs.txt')\n",
    "\n",
    "        # Start continuous ping with timestamps in background\n",
    "        info(\"*** Starting continuous ping from h1 to h2\\n\")\n",
    "        h1.cmd('ping -D -i 0.1 -c 400 10.0.2.1 > h1_ping.log 2>&1 &')  # 400 pings to cover 40s\n",
    "        time.sleep(2)  # Allow ping to start\n",
    "\n",
    "        # Start periodic logging of h1_to_h2_active_port on s1\n",
    "        info(\"*** Starting periodic logging of h1_to_h2_active_port on s1\\n\")\n",
    "        subprocess.Popen(['python3', '-c', '''\n",
    "import subprocess\n",
    "import time\n",
    "for i in range(20):  # Log every 2 seconds for 40 seconds\n",
    "    timestamp = i * 2\n",
    "    output_file = f\"s1_periodic_t{timestamp}.txt\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        subprocess.run(\"echo 'register_read h1_to_h2_active_port' | simple_switch_CLI --thrift-port 9090\", shell=True, stdout=f)\n",
    "    time.sleep(2)\n",
    "'''], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "        # Start background traffic to create congestion\n",
    "        info(\"*** Starting background traffic from h3 to h2\\n\")\n",
    "        h2.cmd('iperf -s -u -p 5001 > h2_iperf_server.log 2>&1 &')  # Background server\n",
    "        time.sleep(1)  # Allow server to start\n",
    "        iperf_cmd = h3.cmd('iperf -c 10.0.2.1 -u -p 5001 -b 25M -t 45 -i 1 > h3_iperf_client.log 2>&1 &')\n",
    "        info(f\"iperf client output: {iperf_cmd}\")\n",
    "        time.sleep(12)  # Allow iperf to generate initial data (12s to reach 14s total)\n",
    "\n",
    "        # Read registers during congestion (pre-failure, at ~14s)\n",
    "        info(\"*** Register State During Congestion (Pre-Failure)\\n\")\n",
    "        read_registers(net, 's1', 9090, 's1_congestion_pre_failure.txt')\n",
    "        read_registers(net, 's2', 9091, 's2_congestion_pre_failure.txt')\n",
    "        read_registers(net, 's3', 9092, 's3_congestion_pre_failure.txt')\n",
    "\n",
    "        # Simulate link failure between s1 and s2 (primary link) at 15s\n",
    "        info(\"\\n*** Simulating link failure between s1 and s2 (primary link)\\n\")\n",
    "        time.sleep(1)  # Ensure we hit 15s\n",
    "        s1 = net.get('s1')\n",
    "        s2 = net.get('s2')\n",
    "        s1_s2_links = net.linksBetween(s1, s2)\n",
    "        info(f\"Links between s1 and s2: {s1_s2_links}\\n\")\n",
    "        primary_link_found = False\n",
    "        for link in s1_s2_links:\n",
    "            info(f\"Checking link: {link.intf1.name} <-> {link.intf2.name}\\n\")\n",
    "            if link.intf1.name == 's1-eth3' and link.intf2.name == 's2-eth3':\n",
    "                info(f\"Disabling primary link: {link}\\n\")\n",
    "                try:\n",
    "                    link.stop()\n",
    "                    time.sleep(0.5)  # Ensure link is fully down\n",
    "                except AttributeError as e:\n",
    "                    error(f\"Failed to stop link {link}: {e}\\n\")\n",
    "                primary_link_found = True\n",
    "                if (execute_switch_cmd(net, 's1', 9090, 'register_write link_status 3 0') and\n",
    "                    execute_switch_cmd(net, 's2', 9091, 'register_write link_status 3 0')):\n",
    "                    info(\"Link failure commands executed successfully\\n\")\n",
    "                else:\n",
    "                    error(\"Link failure commands failed\\n\")\n",
    "                break\n",
    "        if not primary_link_found:\n",
    "            error(\"Primary link s1-eth3 <-> s2-eth3 not found! Listing all links:\\n\")\n",
    "            for link in net.links:\n",
    "                info(f\"Link: {link.intf1.name} <-> {link.intf2.name}\\n\")\n",
    "        time.sleep(25)  # Capture post-failure data until ~40s\n",
    "\n",
    "        # Post-failure state\n",
    "        info(\"*** Post-Failure Register State\\n\")\n",
    "        read_registers(net, 's1', 9090, 's1_post_regs.txt')\n",
    "        read_registers(net, 's2', 9091, 's2_post_regs.txt')\n",
    "        read_registers(net, 's3', 9092, 's3_post_regs.txt')\n",
    "\n",
    "        info(\"*** Testing backup path\\n\")\n",
    "        net.pingAll()\n",
    "\n",
    "        # Display logs\n",
    "        info(\"*** Full h1 ping log:\\n\")\n",
    "        with open('h1_ping.log', 'r') as f:\n",
    "            info(f.read())\n",
    "        info(\"*** h3 to h2 iperf client log:\\n\")\n",
    "        with open('h3_iperf_client.log', 'r') as f:\n",
    "            info(f.read())\n",
    "        info(\"*** h2 iperf server log:\\n\")\n",
    "        with open('h2_iperf_server.log', 'r') as f:\n",
    "            info(f.read())\n",
    "\n",
    "        CLI(net)\n",
    "    finally:\n",
    "        h1.cmd('pkill -f \"ping.*10.0.2.1\"')\n",
    "        h2.cmd('pkill -f \"iperf.*5001\"')\n",
    "        h3.cmd('pkill -f \"iperf.*10.0.2.1\"')\n",
    "        time.sleep(2)  # Ensure processes terminate\n",
    "        try:\n",
    "            net.stop()\n",
    "        except AttributeError as e:\n",
    "            error(f\"Error during net.stop(): {e}\\n\")\n",
    "        os.system('pkill -f \"simple_switch|bmv2\"')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e25ee41-3035-48c1-966b-311542c6111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Parse ping log (already fixed to correctly detect packet loss)\n",
    "def parse_ping_log(file_path):\n",
    "    timestamps, seqs, times = [], [], []\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            expected_seq = 1\n",
    "            for line in lines:\n",
    "                if 'bytes from' in line:\n",
    "                    timestamp = float(re.search(r'\\[(.*?)\\]', line).group(1))\n",
    "                    seq = int(re.search(r'icmp_seq=(\\d+)', line).group(1))\n",
    "                    time = float(re.search(r'time=(\\d+\\.\\d+)', line).group(1))\n",
    "                    while expected_seq < seq:\n",
    "                        if timestamps:\n",
    "                            last_timestamp = timestamps[-1]\n",
    "                            estimated_timestamp = last_timestamp + 0.1 * (expected_seq - seqs[-1])\n",
    "                        else:\n",
    "                            estimated_timestamp = timestamp\n",
    "                        timestamps.append(estimated_timestamp)\n",
    "                        seqs.append(expected_seq)\n",
    "                        times.append(None)\n",
    "                        expected_seq += 1\n",
    "                    timestamps.append(timestamp)\n",
    "                    seqs.append(seq)\n",
    "                    times.append(time)\n",
    "                    expected_seq = seq + 1\n",
    "                elif 'timeout' in line or 'unreachable' in line:\n",
    "                    timestamp = float(re.search(r'\\[(.*?)\\]', line).group(1))\n",
    "                    seq = int(re.search(r'icmp_seq=(\\d+)', line).group(1))\n",
    "                    while expected_seq < seq:\n",
    "                        last_timestamp = timestamps[-1] if timestamps else timestamp\n",
    "                        estimated_timestamp = last_timestamp + 0.1 * (expected_seq - seqs[-1]) if seqs else timestamp\n",
    "                        timestamps.append(estimated_timestamp)\n",
    "                        seqs.append(expected_seq)\n",
    "                        times.append(None)\n",
    "                        expected_seq += 1\n",
    "                    timestamps.append(timestamp)\n",
    "                    seqs.append(seq)\n",
    "                    times.append(None)\n",
    "                    expected_seq = seq + 1\n",
    "        if not timestamps:\n",
    "            raise ValueError(\"No valid ping data found in the log file.\")\n",
    "        timestamps = [t - timestamps[0] for t in timestamps]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} not found. Using dummy ping data.\")\n",
    "        timestamps = list(range(0, 40, 1))\n",
    "        seqs = list(range(1, 41))\n",
    "        times = [7.5 if t < 15 else 25.0 for t in timestamps]\n",
    "    return pd.DataFrame({'timestamp': timestamps, 'seq': seqs, 'time_ms': times})\n",
    "\n",
    "# Parse register file (unchanged)\n",
    "def parse_registers(file_path):\n",
    "    link_status, backup_counter, h1_to_h2_active_port = None, None, None\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if 'link_status=' in line:\n",
    "                    link_status = [int(x) for x in line.split('=')[1].split(',')][:7]\n",
    "                if 'backup_counter=' in line:\n",
    "                    backup_counter = int(line.split('=')[1].strip())\n",
    "                if 'h1_to_h2_active_port=' in line:\n",
    "                    h1_to_h2_active_port = int(line.split('=')[1].strip())\n",
    "        if link_status is None or backup_counter is None:\n",
    "            raise ValueError(f\"Missing required register data in {file_path}.\")\n",
    "        if h1_to_h2_active_port is None:\n",
    "            print(f\"Warning: h1_to_h2_active_port not found in {file_path}. Defaulting to 0.\")\n",
    "            h1_to_h2_active_port = 0\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} not found. Using dummy register data.\")\n",
    "        link_status = [0, 0, 1, 1, 1, 1, 1]\n",
    "        backup_counter = 0\n",
    "        h1_to_h2_active_port = 0\n",
    "    return {\n",
    "        'link_status': link_status,\n",
    "        'backup_counter': backup_counter,\n",
    "        'h1_to_h2_active_port': h1_to_h2_active_port\n",
    "    }\n",
    "\n",
    "# Parse periodic register logs (0.5-second intervals)\n",
    "def parse_periodic_registers(base_filename, num_logs=80):\n",
    "    timestamps = []\n",
    "    active_ports = []\n",
    "    for i in range(num_logs):\n",
    "        timestamp = i * 0.5\n",
    "        file_path = f\"{base_filename}_t{timestamp}.txt\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    if 'h1_to_h2_active_port=' in line:\n",
    "                        active_port = int(line.split('=')[1].strip())\n",
    "                        timestamps.append(timestamp)\n",
    "                        active_ports.append(active_port)\n",
    "                        break\n",
    "                else:\n",
    "                    print(f\"Warning: h1_to_h2_active_port not found in {file_path}. Skipping.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: {file_path} not found. Skipping.\")\n",
    "    if not timestamps:\n",
    "        print(\"Error: No periodic register data found. Using dummy data.\")\n",
    "        timestamps = list(np.arange(0, 40, 0.5))\n",
    "        active_ports = [3 if t < 15 else 5 for t in timestamps]\n",
    "    return pd.DataFrame({'timestamp': timestamps, 'active_port': active_ports})\n",
    "\n",
    "# Parse iperf log for throughput (unchanged)\n",
    "def parse_iperf_log(file_path):\n",
    "    throughput_data = {'timestamp': [], 'throughput': []}\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            print(f\"Raw iperf log content:\\n{content}\")\n",
    "            for line in content.splitlines():\n",
    "                match = re.search(r'\\[\\s*\\d+\\]\\s*(\\d+\\.\\d+-\\d+\\.\\d+)\\s+sec\\s+\\d+\\.\\d+\\s+MBytes\\s+(\\d+\\.\\d+)\\s+Mbits/sec', line)\n",
    "                if match:\n",
    "                    start_time = float(match.group(1).split('-')[0])\n",
    "                    throughput_val = float(match.group(2))\n",
    "                    throughput_data['timestamp'].append(start_time)\n",
    "                    throughput_data['throughput'].append(throughput_val)\n",
    "                else:\n",
    "                    match_alt = re.search(r'(\\d+\\.\\d+)\\s+Mbits/sec', line)\n",
    "                    if match_alt:\n",
    "                        throughput_data['throughput'].append(float(match_alt.group(1)))\n",
    "                        throughput_data['timestamp'].append(len(throughput_data['timestamp']) * 1.0)\n",
    "        if not throughput_data['throughput']:\n",
    "            print(\"No throughput data found in log. Using dummy data for debugging.\")\n",
    "            throughput_data['timestamp'] = [0, 5, 10, 15, 20, 25, 30]\n",
    "            throughput_data['throughput'] = [25, 24, 23, 15, 14, 13, 12]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} not found. Using dummy data.\")\n",
    "        throughput_data['timestamp'] = [0, 5, 10, 15, 20, 25, 30]\n",
    "        throughput_data['throughput'] = [25, 24, 23, 15, 14, 13, 12]\n",
    "    return pd.DataFrame(throughput_data)\n",
    "\n",
    "# Calculate packet loss rate over time\n",
    "def calculate_packet_loss_rate(ping_data, window_size=1.0):\n",
    "    ping_data = ping_data.sort_values('seq')\n",
    "    timestamps = ping_data['timestamp'].values\n",
    "    seqs = ping_data['seq'].values\n",
    "    times = ping_data['time_ms'].values\n",
    "\n",
    "    expected_seqs = np.arange(seqs[0], seqs[-1] + 1)\n",
    "    actual_seqs = set(seqs)\n",
    "    missing_seqs = [seq for seq in expected_seqs if seq not in actual_seqs]\n",
    "\n",
    "    loss_timestamps = []\n",
    "    for missing_seq in missing_seqs:\n",
    "        before_idx = np.where(seqs < missing_seq)[0][-1] if np.any(seqs < missing_seq) else None\n",
    "        after_idx = np.where(seqs > missing_seq)[0][0] if np.any(seqs > missing_seq) else None\n",
    "        if before_idx is not None and after_idx is not None:\n",
    "            t_before = timestamps[before_idx]\n",
    "            t_after = timestamps[after_idx]\n",
    "            s_before = seqs[before_idx]\n",
    "            s_after = seqs[after_idx]\n",
    "            t_missing = t_before + (t_after - t_before) * (missing_seq - s_before) / (s_after - s_before)\n",
    "            loss_timestamps.append(t_missing)\n",
    "        elif before_idx is not None:\n",
    "            t_before = timestamps[before_idx]\n",
    "            loss_timestamps.append(t_before + 0.1)\n",
    "        elif after_idx is not None:\n",
    "            t_after = timestamps[after_idx]\n",
    "            loss_timestamps.append(t_after - 0.1)\n",
    "\n",
    "    max_time = timestamps[-1]\n",
    "    bins = np.arange(0, max_time + window_size, window_size)\n",
    "    sent_counts, _ = np.histogram(timestamps, bins=bins)\n",
    "    loss_counts, _ = np.histogram(loss_timestamps, bins=bins)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "    total_packets = sent_counts + loss_counts\n",
    "    loss_rates = np.where(total_packets > 0, loss_counts / total_packets, 0) * 100\n",
    "    return bin_centers, loss_rates, sent_counts, loss_counts\n",
    "\n",
    "# Load data\n",
    "ping_data = parse_ping_log('h1_ping.log')\n",
    "s1_initial = parse_registers('s1_initial_regs.txt')\n",
    "s1_congestion = parse_registers('s1_congestion_pre_failure.txt')\n",
    "s1_post = parse_registers('s1_post_regs.txt')\n",
    "periodic_data = parse_periodic_registers('s1_periodic', num_logs=80)\n",
    "iperf_data = parse_iperf_log('h3_iperf_client.log')\n",
    "\n",
    "# Failure time and reroute time\n",
    "failure_time = 15.0\n",
    "reroute_time = 16.0  # Based on the packet loss rate spike ending at 16 seconds\n",
    "\n",
    "# Calculate packet loss rate\n",
    "window_size = 1.0\n",
    "loss_timestamps, loss_rates, sent_counts, loss_counts = calculate_packet_loss_rate(ping_data, window_size)\n",
    "\n",
    "# 1. Combined Latency, Port Usage, and Packet Loss Rate Plot\n",
    "fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Latency Plot with traffic flow indication (left y-axis)\n",
    "pre_failure = ping_data[ping_data['timestamp'] < failure_time]\n",
    "transition = ping_data[(ping_data['timestamp'] >= failure_time) & (ping_data['timestamp'] < reroute_time)]\n",
    "post_failure = ping_data[ping_data['timestamp'] >= reroute_time]\n",
    "ax1.plot(pre_failure['timestamp'], pre_failure['time_ms'], 'b-', label='Ping Latency (Primary Path)', linewidth=1.5)\n",
    "ax1.plot(transition['timestamp'], transition['time_ms'], 'orange', label='Ping Latency (Transition)', linewidth=1.5)\n",
    "ax1.plot(post_failure['timestamp'], post_failure['time_ms'], 'c-', label='Ping Latency (Backup Path)', linewidth=1.5)\n",
    "ping_data['time_ms_smooth'] = ping_data['time_ms'].interpolate(method='linear')\n",
    "ax1.plot(ping_data['timestamp'], ping_data['time_ms_smooth'], 'b--', alpha=0.5)\n",
    "\n",
    "# Packet Loss Markers\n",
    "loss_points = ping_data[ping_data['time_ms'].isna()]\n",
    "if not loss_points.empty:\n",
    "    ax1.scatter(loss_points['timestamp'], [7.5] * len(loss_points), color='red', marker='x', s=100, label='Packet Loss')\n",
    "else:\n",
    "    ax1.text(failure_time + 1, 5, 'No Packet Loss Observed', color='red', fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "# Packet Loss Rate (left y-axis, scaled to fit with latency)\n",
    "# Scale the packet loss rate to fit within the latency range (0–30 ms)\n",
    "scaled_loss_rates = loss_rates * (30 / 100)  # Scale 0–100% to 0–30 ms range\n",
    "ax1.fill_between(loss_timestamps, 0, scaled_loss_rates, color='red', alpha=0.2, label='Packet Loss Rate (%)')\n",
    "\n",
    "ax1.set_ylabel('Latency (ms) / Scaled Packet Loss Rate', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "ax1.set_title('Combined Ping Latency, Port Usage, and Packet Loss Rate Over Time')\n",
    "ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "ax1.set_ylim(0, 30)\n",
    "ax1.axvline(failure_time, color='r', linestyle='--', label='Link Failure')\n",
    "ax1.annotate('Link Failure', xy=(failure_time, 15), xytext=(failure_time + 2, 20),\n",
    "             arrowprops=dict(arrowstyle='->'), color='r')\n",
    "\n",
    "# Highlight the transition window\n",
    "ax1.axvspan(failure_time, reroute_time, color='yellow', alpha=0.2, label='Failure to Reroute Window')\n",
    "\n",
    "# Add shaded regions for traffic flow\n",
    "ax1.axvspan(0, failure_time, color='blue', alpha=0.05, label='Traffic on Primary Path (Port 3)')\n",
    "ax1.axvspan(reroute_time, ping_data['timestamp'].max(), color='cyan', alpha=0.05, label='Traffic on Backup Path (Port 5)')\n",
    "\n",
    "# Active Port Plot (right y-axis)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.step(periodic_data['timestamp'], periodic_data['active_port'], 'g-', where='post', label='Active Port', linewidth=1.5)\n",
    "ax2.set_ylabel('Active Port', color='g')\n",
    "ax2.tick_params(axis='y', labelcolor='g')\n",
    "ax2.set_ylim(2, 6)\n",
    "\n",
    "# Annotate the port switch\n",
    "switch_idx = periodic_data['active_port'].ne(periodic_data['active_port'].shift()).idxmax()\n",
    "switch_time = periodic_data['timestamp'].iloc[switch_idx] if switch_idx < len(periodic_data) else failure_time\n",
    "switch_port = periodic_data['active_port'].iloc[switch_idx] if switch_idx < len(periodic_data) else 5\n",
    "ax2.annotate(f'Port Switch to {switch_port}', xy=(switch_time, switch_port), xytext=(switch_time + 2, switch_port + 0.5),\n",
    "             arrowprops=dict(arrowstyle='->'), color='g')\n",
    "\n",
    "# Combine legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', frameon=True, edgecolor='black', fontsize=10)\n",
    "\n",
    "ax1.set_xlabel('Time (seconds)')\n",
    "plt.tight_layout(pad=2.0)\n",
    "plt.savefig('combined_rerouting_plot.png')\n",
    "plt.show()\n",
    "\n",
    "# 2. Throughput Plot with Moving Average (unchanged)\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "if not iperf_data.empty and 'timestamp' in iperf_data.columns and 'throughput' in iperf_data.columns:\n",
    "    pre_failure_iperf = iperf_data[iperf_data['timestamp'] < failure_time]\n",
    "    post_failure_iperf = iperf_data[iperf_data['timestamp'] >= failure_time]\n",
    "    ax.plot(pre_failure_iperf['timestamp'], pre_failure_iperf['throughput'], 'm-', label='Throughput (Pre-Failure)', linewidth=1.5)\n",
    "    ax.plot(post_failure_iperf['timestamp'], post_failure_iperf['throughput'], 'purple', label='Throughput (Post-Failure)', linewidth=1.5)\n",
    "    iperf_data['throughput_ma'] = iperf_data['throughput'].rolling(window=3, min_periods=1).mean()\n",
    "    ax.plot(iperf_data['timestamp'], iperf_data['throughput_ma'], 'k--', label='Throughput (Moving Avg)', linewidth=1.5)\n",
    "else:\n",
    "    print(\"Warning: No valid iperf data to plot. Skipping throughput plot.\")\n",
    "ax.axvline(failure_time, color='r', linestyle='--', label='Link Failure')\n",
    "ax.annotate('Link Failure', xy=(failure_time, 15), xytext=(failure_time + 2, 20),\n",
    "            arrowprops=dict(arrowstyle='->'), color='r')\n",
    "ax.set_xlabel('Time (seconds)')\n",
    "ax.set_ylabel('Throughput (Mbits/sec)')\n",
    "ax.tick_params(axis='y')\n",
    "ax.set_title('Throughput from h3 to h2 During Rerouting')\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "ax.set_ylim(0, 30)\n",
    "ax.legend(loc='upper right', frameon=True, edgecolor='black', fontsize=10)\n",
    "plt.tight_layout(pad=2.0)\n",
    "plt.savefig('throughput_plot.png')\n",
    "plt.show()\n",
    "\n",
    "# 3. Enhanced Register Plot with Link Status and Backup Counter (unchanged)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "stages = ['Initial', 'Congestion (Pre-Failure)', 'Post-Failure']\n",
    "link_status_s1 = [s1_initial['link_status'][3], s1_congestion['link_status'][3], s1_post['link_status'][3]]\n",
    "backup_counters = [s1_initial['backup_counter'], s1_congestion['backup_counter'], s1_post['backup_counter']]\n",
    "\n",
    "ax1.bar(stages, link_status_s1, color='purple', alpha=0.5, label='Primary Link Status (1=Up, 0=Down)')\n",
    "ax1.set_xticks(range(len(stages)))\n",
    "ax1.set_xticklabels(stages)\n",
    "ax1.set_ylabel('Link Status')\n",
    "ax1.set_title('s1 Primary Link Status Over Time')\n",
    "ax1.legend()\n",
    "for i, v in enumerate(link_status_s1):\n",
    "    ax1.text(i, v + 0.05, str(v), ha='center', fontsize=10)\n",
    "\n",
    "ax2.bar(stages, backup_counters, color='orange', alpha=0.5, label='Backup Counter')\n",
    "ax2.set_xticks(range(len(stages)))\n",
    "ax2.set_xticklabels(stages)\n",
    "ax2.set_ylabel('Backup Counter')\n",
    "ax2.set_title('s1 Backup Counter Over Time')\n",
    "ax2.legend()\n",
    "for i, v in enumerate(backup_counters):\n",
    "    ax2.text(i, v + 0.5, str(v), ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout(pad=2.0)\n",
    "plt.savefig('register_states_enhanced.png')\n",
    "plt.show()\n",
    "\n",
    "# 4. Cumulative Packet Loss Plot (unchanged)\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.plot(cumulative_timestamps, cumulative_loss, 'r-', label='Cumulative Packet Loss', linewidth=1.5)\n",
    "ax.set_xlabel('Time (seconds)')\n",
    "ax.set_ylabel('Cumulative Packet Loss (Packets)', color='r')\n",
    "ax.tick_params(axis='y', labelcolor='r')\n",
    "ax.set_title('Cumulative Packet Loss Over Time')\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "ax.axvline(failure_time, color='r', linestyle='--', label='Link Failure')\n",
    "ax.annotate('Link Failure', xy=(failure_time, cumulative_loss[-1] / 2), xytext=(failure_time + 2, cumulative_loss[-1] / 2 + 1),\n",
    "            arrowprops=dict(arrowstyle='->'), color='r')\n",
    "ax.axvspan(failure_time, reroute_time, color='yellow', alpha=0.2, label='Failure to Reroute Window')\n",
    "ax.legend(loc='upper left', frameon=True, edgecolor='black', fontsize=10)\n",
    "plt.tight_layout(pad=2.0)\n",
    "plt.savefig('cumulative_packet_loss_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce38776c-4948-4163-b5db-920df415e60f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
